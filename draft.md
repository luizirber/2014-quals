---
title: Overlap graph-based sequence assembly in bioinformatics
author:
- name: Luiz Carlos Irber Jr
  affiliation: Michigan State University
  email: irberlui@msu.edu
date: August 2014
abstract: ""
bibliography: <!-- \bibliography{bibs/qualsbib-pandoc.bib} -->
...

# Introduction

The genome sequencing purpose is to find the nucleotide sequence of
a long organic molecule (a strand of DNA, for example).
<!-- DNA molecules -->

This long organic molecule is prepared for sequencing using chemical processes
to make it easier to be examined by a genome sequencer.
The genome sequencer process the molecule and generate a read,
a string of characters representing the underlying nucleotide sequence,
usually using "AGCT" (for Adenine, Guanine, Cytosine and Thiamine) as an alphabet.

Since no current technology can examine a molecule from beginning to end,
during the preparation the target molecule is replicated and broken in smaller fragments.
After processing each piece is output as a read by the sequencer,
with sizes ranging from 20 to more than 15K basepairs,
depending on the technology.
These reads need to be assembled to represent the original molecule's nucleotide sequence.

## The genome assembly problem

The genome assembly problem consists in reconstructing the original sequence
using the set of reads generated by a genome sequencer.
There are different theoretical approaches to solve the problem,
but usually in practice an approximation is returned
if the exact solution can't be achieved.

An assembler is a computer program which implements a solution to this problem,
using the set of reads as input and outputting the original sequence.

But reads are rarely perfect.
Errors occurring during the nucleotide identification can lead to
insertions, deletions or substitutions of the original sequence.
Also biological idiosyncrasies,
like highly repetitive or high GC-content regions,
can be difficult to sequence or not captured at all in the resulting reads.

These practical problems leads to suboptimal solutions...
contigs are continuous sequences of basepairs representing part or,
although rare, all the original biological sequence.

Genome sequencing technologies throughput is increasing steadily over the years
and is usually achieved by generating shorter reads.
This poses additional challenges for assemblers,
since scaling to handle millions (or even billions) of short reads
involves more than performance optimization.

# Theoretical frameworks

## The greedy method

This method was used mainly by the first assemblers for Sanger sequencing,
and it represents the genome assembly problem as a
shortest common superstring (SCS) problem:
given a set of reads, find the shortest string containing all reads as substrings.

The greedy method is based on finding all the overlaps between reads and
classify them using a scoring function,
based on criteria like overlap length or identity, for example.
Two reads with high scoring overlap are selected as seed and extended
successively with other overlapping reads until there are no overlaps left.

Shortcomings of this approach involve:

  - Misassembly of repeats. Since repeats will extend the original sequence,
    the greedy method tends to collapse repetitive regions,
    disagreeing with the original sequenced molecule.
  - Misplaced reads in contigs. Due to repetitive regions some reads might end
    in the wrong contig,
    misrepresenting the right order of the original sequence.

For bacterial genomes the results of this approach are acceptable,
since a smaller part of their genome is composed of repetitive regions and
also their genomes tend to be small.
But for eukaryotes the method produces incorrect contigs,
and since it involves multiple comparisons steps they don't scale well.

## Overlap-Layout-Consensus approaches

Conceptualizes the genome assembly in three steps:

 - Overlap: calculate all approximate overlaps between reads.

<!--
   There are four types of overlaps:
    * Containment
    * regular dovetail
    * prefix dovetail
    * suffix dovetail
-->

 - Layout: use the overlap information to decide in which order the reads
   should be joined.

 - Consensus: join reads into contigs, correcting errors if needed.

OLC approaches share this common framework,
but implement it in different ways.
The essence is using some kind of graph for the layout step,
built with the overlap step information.
During the consensus step the graph traversed,
outputting contigs.
Note that the greedy method can be considered a form of OLC,
where the layout and the consensus steps are executed at the same time.

## Overlap and Unitig Graphs

The shortest common superstring (SCS) problem is not a good representation of the
genome assembly problem when repeats are considered,
since repeats longer than the read length are collapsed.
[@myers1995toward] proposes another formulation:
calculate a Hamiltonian path (HP) of the overlap graph.
Both HP and SCS are NP-complete,
but this formulation represents better the characteristics of experimental
data (like repeats) and is amenable to simplifications,
trading accuracy for acceptable results (?!?!)

After calculating the overlaps the graph is generated by turning
reads into vertices and adding edges representing the overlaps between reads.
Some overlap graph properties can be exploited to simplify
it while keeping all the original information.
This is important to reduce the combinatorial complexity of the problem and make
it tractable.
The properties are:

  - Contained reads: some reads are substrings of other reads,
    and they can be removed while changing the solution space.
    In practice,
    these reads are saved in an auxiliary list,
    in case they are needed to improve the tentative layout or during the consensus step.
  - Transitive edge removal: If there are reads $f$, $g$ and $h$,
    with overlap edges $f \rightleftharpoons g$, $g \rightleftharpoons h$ and $f \rightleftharpoons h$,
    the overlap edge $f \rightleftharpoons h$ can be removed,
    since $f \rightleftharpoons g \rightleftharpoons h$ spells the same sequence.
  - Collapse reads: if all edges adjacent to vertex $f$,
    except the overlap edge with vertex $g$,
    point to the same direction,
    and vice-versa for $g$,
    vertices $f$ and $g$ can be joined.
    This new vertex represents a chunk or unitig,
    a concatenation of reads.

Due to these simplifications reads from repetitive regions are collapsed in
chunks and need to be traversed multiple times.
This implies the path corresponding to the assembly is not Hamiltonian anymore,
but in practice it is rare to assemble all contigs into the same string,
so it is a valid trade-off to increase the problem tractability.

## de Bruijn approaches

During the last fifteen years a shift occurred in sequencing technologies.
First generation sequencing technologies (Sanger) created reads with length
ranging from 400 to 900 basepairs,
but they were costly to sequence and the throughput was low.
Next-gen sequencing technologies (NGS) use distinct chemical processes and techniques,
but they share the trend to increase throughput by outputting shorter reads at a increased rate.
This led to huge price drops,
but greatly increased the computational resources needed to assemble the reads.

First proposed by [@idury1995new] and later complemented by [@pevzner2001eulerian],
which provided a practical framework and dealt with error correction and repeat solving strategies,
a de Bruijn graph uses a different representation.
Reads are broken in $k$-mers,
and for each $k$-mer two vertices $f$ and $g$ are added to the graph,
each assigned one of the $(k-1)$-mers of the $k$-mer.
A directed edge connect $f$ to $g$,
meaning the last $k-1$ nucleotides of $f$ are the same as the $k-1$ nucleotides of $g$.
The original $k$-mer can be retrieved by concatenating the first nucleotide of
$f$ and all nucleotides of $g$,
or all nucleotides of $f$ and the last nucleotide of $g$.
The original read is represented by a path,
and the same logic to retrieve a $k$-mer can be applied along a path to recreate the read.

One of the benefits of a de Bruijn graph representation is the avoidance of
the overlap calculations step,
which was a drawback for OLC approaches when NGS technologies became available.
Another benefit is a new formulation for the genome assembly problem,
since the solution is now finding an Eulerian path,
a graph problem with polynomial time solution.

In practice read errors increase the number of misleading paths,
and there are paths represent invalid reads.
More so,
short repeats create tangles which are unsolvable if no read covers them.
With these conditions calculating an Eulerian path is hard or not possible,
and a solution is to generalize the problem to a Eulerian superpath problem (ESP):
given an Eulerian graph $G$ and a collection of paths $P$,
find an Eulerian path containing all subpaths in $P$.
The ESP is a NP-complete problem.
[@pevzner2001eulerian] discusses how to solve this problem in detail,
taking advantage of equivalent transformations aiming to simplify the graph,
and in many cases the problem can be reduced again to an Eulerian path problem,
but there are conditions when the problem is unresolvable.

## String Graph

De Bruijn graphs approaches take advantage of the inherent compression of
using $k$-mers for read representation,
but introduces new problems related to coherence of paths and reads.
[@myers2005fragment] proposes another representation for overlap graphs: the string graph.

A string graph represents the start and the end of a read as vertices,
an each edge represents the non-overlapped parts of the overlap of two reads.
<!-- add bidirected edges here! -->
The string graph construction process shares some similarities with the unitig graph,
involving contained reads removal and transitive edge reduction,
but instead of applying a vertices collapsing strategy to create unitigs
a more refined procedure is applied.
This involves estimating traversal counts for each edge using the $A$-statistic,
which relates the probabilities that the path is a single copy and that it
should be transversed twice,
and mapping previously removed contained reads to avoid imprecisions on the
traversal count estimation.

To calculate the A-statistic the vertex are classified in junction and internal vertices.
A junction vertex has in- or out-degree different than 1,
and vertices not satisfying this condition are internal vertices.
With this information the A-statistics is defined by
$$A(\Delta, k) = \Delta(\frac{n}{G}) - k \ln 2$$
where:

- $G$: size of the genome
- $n$: number of reads
- $\Delta$: length of a composite edge between two junction vertices
- $k$: number of internal vertices of the composite edge

<!-- explain min flow for traversal count
First suggested by [@pevzner2001eulerian],

Based on the A-statistic estimations it is possible to decide the traversal
count

  - Traversal count for each edge by solving a
    minimum cost network flow problem.
  - Drop edges with zero count (false overlaps)
-->

# Practical assemblers

Despite efforts to model the genome assembly problem and the theoretical
framework they provide,
many practical problems lead to suboptimal solutions when applied to
reads generated with current sequencing technologies.
Assemblers need to deal with accuracy and tractability trade-offs to
create acceptable solutions,
and this section discusses four assemblers and how they implement the
theoretical approaches,
how they adapt to deal with inconsistencies
and performs other forms of error correction.

## Velvet

[@zerbino2008velvet]
  - Techniques
    * de Bruijn graph
    * Tour bus error correction algorithm
      - Dijkstra-like BFS, implemented with a Fibonnaci heap: O(N logN)
      - 
    * Breadcrumb
      - Use paired end data to resolve repeats
  - Pipeline:
    * Hash the reads into k-mers
      - Roadmaps
    * Build the dBG using the roadmaps
      - Simplification:
        if node A has only one outgoing arc to node B
        and node B has only one ingoing arc
        merge the two nodes (and their twins),
        transfering arc, read and sequence information as appropriate
    * Error removal
      - Removal of low coverage nodes. Remove errors, but:
        * Are they genuine errors os biological variants?
        * Errors need to be randomly distributed in reads
          (not true for most sequencing technologies)
      - Resolve topological features:
        * tips
        * bulges / bubbles
        * erroneous connections
    * Resolution of repeats
      - with short read pairs

## SGA

[@simpson2012efficient]

 - String graph
 - FM-index (full-text minute-space index), derived from
   compressed BWT (Burrows-Wheeler transform)
   * Compressed representation of the full set of sequence reads

 - Implicit overlap graph (from FM index)
 - Fast overlap calculation
   * FM index

 - Error correction
   * kmer frequency
     - sga
   * unique kmers
     - remove (sga)
   * Remove tips (vertex only have one connection in one direction)


## ReadJoiner

[@gonnella2012readjoiner]

  - Techniques:
    * integer encoding
    * suffix sorting
    * integer sorting
    * binary search
    * bottom-up traversal of lcp-interval trees
      - lcp: longest common prefix

  - Avoid random access, explore data locality

  - Replace maximality constraint with a
    minimum length constraint imposed on each suffix-prefix match.
    (like Simpson and Durbin)

  - Partition: increase running time, but large reduction in
    overall memory peak

  - Pipeline

    * prefilter: remove reads
      - containing ambiguity codes
      - are prefixes or suffixes of other reads

    * overlap:
      - map prefiltered reads in memory
      - enumerate non-redundant irreducible suffix-prefix matches

    * assembly: build string graph and traverses it to output the contigs.


 - Implicit overlap graph (from LCP trees)
 - Fast overlap calculation
   * LCP trees

## Fermi

[@li2012exploring]



# Future research

<!-- 
  - fm-index -> exact overlap match. how to deal with inexact?
  - 
-->
# Conclusion

# References
